{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pedestrian detection  implement.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ky_P87ohN3FM",
        "outputId": "7d260987-2bb9-4b6e-c406-e4da2a3b6c73"
      },
      "source": [
        "!git clone https://github.com/subrahmanya01/pedestrian_intention_detection"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'padestrian_intension_detection' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX5RzQf3U87f",
        "outputId": "152b7894-f23d-493f-86ab-3d063a91b8e8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvVmsEtAcnW7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d5733ca-1047-4e7d-a92d-cc3e311c2894"
      },
      "source": [
        "pip install scikit-learn==0.22.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.22.2\n",
            "  Downloading scikit_learn-0.22.2-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2) (1.4.1)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.22.2 which is incompatible.\u001b[0m\n",
            "Successfully installed scikit-learn-0.22.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To clear the all defined flags\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()\n",
        "    keys_list = [keys for keys in flags_dict]\n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "del_all_flags(flags.FLAGS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "HJDi-QIjxKwR",
        "outputId": "5dd2951e-df26-41e0-d88e-f6546c2fb5ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-17e5b2d4e9e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__delattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdel_all_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'flags' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7Eu-9UdPUGT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58fae2ff-555b-4bc8-caf7-f03822e40aad"
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import glob\n",
        "\n",
        "%cd /content/pedestrian_intention_detection\n",
        " \n",
        "import sys \n",
        "from absl import app, logging, flags\n",
        "from absl.flags import FLAGS\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from yolov3_tf2.models import YoloV3\n",
        "from yolov3_tf2.dataset import transform_images, load_tfrecord_dataset\n",
        "from yolov3_tf2.utils import draw_outputs\n",
        "\n",
        "%cd /content/pedestrian_intention_detection/deep_sort\n",
        "from ds_tools.generate_detections import create_box_encoder\n",
        "from ds_application_util import preprocessing\n",
        "from ds_deep_sort import nn_matching\n",
        "from ds_deep_sort.detection import Detection\n",
        "from ds_deep_sort.tracker import Tracker\n",
        "\n",
        "\n",
        "%cd /content/pedestrian_intention_detection\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "flags.DEFINE_string('classes', '/content/drive/MyDrive/yolov3/coco.names', 'path to classes file')\n",
        "flags.DEFINE_string('weights', '/content/drive/MyDrive/datax_volvo_additional_files/yolov3_train_5.tf','path to weights file')\n",
        "flags.DEFINE_boolean('tiny', False, 'yolov3 or yolov3-tiny')\n",
        "flags.DEFINE_integer('size', 416, 'resize images to')\n",
        "flags.DEFINE_string('tfrecord', None, 'tfrecord instead of image')\n",
        "flags.DEFINE_integer('num_classes', 1, 'number of classes in the model')\n",
        "flags.DEFINE_string('video', '/content/drive/MyDrive/yolov3/video_0346.mp4','path to video file or number for webcam)')\n",
        "flags.DEFINE_string('output','/content/drive/MyDrive/yolov3/output2_video.mp4', 'path to output video')\n",
        "flags.DEFINE_string('output_format', 'mp4v', 'codec used in VideoWriter when saving video to file')\n",
        "\n",
        "app._run_init(['yolov3'], app.parse_flags_with_usage)\n",
        "\n",
        "with open('densenet_model.json', 'r') as json_file:\n",
        "    json_savedModel= json_file.read()\n",
        "\n",
        "model_j = tf.keras.models.model_from_json(json_savedModel)\n",
        "model_j.load_weights('densenet_1.hdf5')\n",
        "\n",
        "def pred_func(X_test):\n",
        "  predictions = model_j.predict(X_test[0:1], verbose=0)\n",
        "  Y = np.argmax(predictions[0], axis=0)\n",
        "    \n",
        "  return Y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/padestrian_intension_detection\n",
            "/content/padestrian_intension_detection/deep_sort\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "/content/padestrian_intension_detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:552: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`normal` is a deprecated alias for `truncated_normal`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0105 05:59:37.862684 140229009536896 deprecation.py:619] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:552: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`normal` is a deprecated alias for `truncated_normal`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/normalization/batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0105 05:59:38.065458 140229009536896 deprecation.py:347] From /usr/local/lib/python3.7/dist-packages/keras/layers/normalization/batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Mfr73upckWY"
      },
      "source": [
        "# parameters for DeepSORT the object tracker\n",
        "nms_max_overlap = 1.0\n",
        "max_cosine_distance = 0.2\n",
        "nn_budget = None\n",
        "\n",
        "# DeepSORT initialization\n",
        "encoder = create_box_encoder('mars-small128.pb', batch_size=32)\n",
        "metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
        "tracker = Tracker(metric)\n",
        "\n",
        "# Yolo initialization\n",
        "FLAGS.yolo_iou_threshold = 0.5\n",
        "FLAGS.yolo_score_threshold = 0.5\n",
        "\n",
        "yolo = YoloV3(classes=FLAGS.num_classes)\n",
        "yolo.load_weights(FLAGS.weights).expect_partial()\n",
        "\n",
        "class_names = [c.strip() for c in open(FLAGS.classes).readlines()]\n",
        "\n",
        "resize_out_ratio = 4.0\n",
        "fps_time = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdR-4X1FPxKg"
      },
      "source": [
        "\n",
        "\n",
        "def run_model():\n",
        "  print('Processing started.......')\n",
        "  try:\n",
        "      vid = cv2.VideoCapture(int(FLAGS.video))\n",
        "  except:\n",
        "      vid = cv2.VideoCapture(FLAGS.video)\n",
        "\n",
        "  out = None\n",
        "  length = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "  if FLAGS.output:\n",
        "      # by default VideoCapture returns float instead of int\n",
        "      width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "      height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "      fps = int(vid.get(cv2.CAP_PROP_FPS))\n",
        "      codec = cv2.VideoWriter_fourcc(*FLAGS.output_format)\n",
        "      out = cv2.VideoWriter(FLAGS.output, codec, fps, (width, height))\n",
        "\n",
        "  frame = 0\n",
        "  rolling_data={}\n",
        "  fps_time=0\n",
        "  result=[]\n",
        "  kt=True\n",
        "  while True:\n",
        "\n",
        "    _, img = vid.read() # reading the image\n",
        "\n",
        "    if img is None:\n",
        "        break\n",
        "        logging.warning(\"Empty Frame\")\n",
        "        time.sleep(0.1)\n",
        "        continue\n",
        "    completion(frame,length)\n",
        "    frame = frame+ 1   \n",
        "    currFrame = int(vid.get(cv2.CAP_PROP_POS_FRAMES))\n",
        "    fps = vid.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "\n",
        "    img_in = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
        "    img_orig = np.copy(img_in)\n",
        "    img_in = tf.expand_dims(img_in, 0)\n",
        "    img_in = transform_images(img_in, FLAGS.size)\n",
        "\n",
        "    boxes, scores, classes, nums = yolo.predict(img_in, steps=1) # yolo\n",
        "    boxes = boxes[:,:nums[0],:].reshape(nums[0], 4)[classes[0][:nums[0]] == 0]\n",
        "    scores = scores[0][:nums[0]][classes[0][:nums[0]] == 0]\n",
        "    nums = len(boxes)\n",
        "\n",
        "    # converting [x1,y1,x2,y2] -> [x1,y1,w,h] for boxes detected\n",
        "    wh = np.flip(img.shape[0:2])\n",
        "    bbtlwh = []\n",
        "    for i in range(nums):\n",
        "\n",
        "      x1y1 = tuple((np.array(boxes[i][0:2]) * wh).astype(np.int32))\n",
        "      x1 = x1y1[0]\n",
        "      y1 = x1y1[1]\n",
        "      x2y2 = tuple((np.array(boxes[i][2:4]) * wh).astype(np.int32))\n",
        "      bbwh = (x2y2[0]-x1y1[0], x2y2[1]-x1y1[1])\n",
        "      w = bbwh[0]\n",
        "      h = bbwh[1]\n",
        "      bbtlwh.append([x1,y1,w,h])\n",
        "\n",
        "    features = encoder(img, bbtlwh) # deepsort input\n",
        "    detections = [Detection(box, conf, feat) for box, conf, feat in zip(bbtlwh, scores, features)] #deep sort output \n",
        "\n",
        "\n",
        "    # Update tracker.\n",
        "    tracker.predict()\n",
        "    tracker.update(detections)\n",
        "    \n",
        "    tracked_bbox = []\n",
        "    ids = []\n",
        "\n",
        "    for track in tracker.tracks:\n",
        "\n",
        "      if not track.is_confirmed() or track.time_since_update > 1:\n",
        "        continue\n",
        "      tracked_bbox.append(track.to_tlwh())\n",
        "      ids.append(track.track_id)\n",
        "\n",
        "\n",
        "    for i in range(len(tracked_bbox)): # densenet \n",
        "\n",
        "      # Show tracker output\n",
        "      x, y, w, h = tracked_bbox[i]\n",
        "      x = int(x)  \n",
        "      y = int(y)\n",
        "      w = int(w) \n",
        "      h = int(h) \n",
        "\n",
        "      # looking for previous 16 frames data for a given pedestrian:\n",
        "\n",
        "      intent = 0 #(default, the pedestrian is not crossing)\n",
        "\n",
        "      \n",
        "      if int(ids[i]) in list(rolling_data.keys()):\n",
        "\n",
        "        if len(rolling_data[int(ids[i])]) == 16:\n",
        "          \n",
        "          seq = np.stack(np.array(rolling_data[int(ids[i])]),axis=2)\n",
        "          seq = np.expand_dims(seq, axis=0)\n",
        "          intent = pred_func(seq) # classification output\n",
        "\n",
        "        else:\n",
        "\n",
        "          seq = np.stack(np.array([rolling_data[int(ids[i])][-1]] * 16),axis=2)\n",
        "          seq = np.expand_dims(seq, axis=0)\n",
        "          intent = pred_func(seq) # classification output\n",
        "\n",
        "      # risky pedestrian identification thru box color\n",
        "\n",
        "      if intent == 1:\n",
        "        color = (0, 0, 255) # Red -> Crossing\n",
        "\n",
        "      else:\n",
        "        color = (0, 255, 0) # green -> Not crossing\n",
        "\n",
        "      fps_time = time.time()\n",
        "      #color = (0, 255, 0)\n",
        "      img = cv2.rectangle(img, (int(x), int(y)), (int(x + w), int(y + h)), color, 2)\n",
        "      img = cv2.putText(img, 'TrackID ' + str(ids[i]), (x, y - 5), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(255, 0, 0), thickness=2)\n",
        "      img = cv2.putText(img,\"Frame No: %d\" % (frame),(10, 10),  cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0, 0, 255), 2)\n",
        "      result.append([frame, int(ids[i]), x, y, w+x,h+y, intent])\n",
        "\n",
        "      # storing the data for last 16 frames\n",
        "      try:\n",
        "\n",
        "        if int(ids[i]) in list(rolling_data.keys()): # ID exists in dict\n",
        "\n",
        "          if len(rolling_data[int(ids[i])]) < 16: # bboxes values for 16 frames\n",
        "              \n",
        "            cropped_seq = []\n",
        "            cropped_img = cv2.resize(img_orig[y:h+y, x:w+x],(100,100))\n",
        "            rolling_data[int(ids[i])].append(np.asarray(cropped_img)) # append the image      \n",
        "\n",
        "          else:\n",
        "\n",
        "            del rolling_data[int(ids[i])][0] # delete oldest frame bbox and append latest frame bbox\n",
        "            cropped_seq = []\n",
        "            cropped_img = cv2.resize(img_orig[y:h+y, x:w+x],(100,100))\n",
        "            rolling_data[int(ids[i])].append(np.asarray(cropped_img))\n",
        "              \n",
        "        else:\n",
        "\n",
        "          cropped_seq = []\n",
        "          cropped_img = cv2.resize(img_orig[y:h+y, x:w+x],(100,100))\n",
        "          rolling_data[int(ids[i])] = [np.asarray(cropped_img)]  \n",
        "\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    \n",
        "    if FLAGS.output:\n",
        "      out.write(img)\n",
        "\n",
        "    if cv2.waitKey(1) == ord('q'):\n",
        "      break\n",
        "\n",
        "  cv2.destroyAllWindows()\n",
        "  print('\\nProcessing completed.......!!!')\n",
        "  print('Check video file in google drive folder!')\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output \n",
        "\n",
        "def completion(Frame,length):\n",
        "  clear_output()\n",
        "  print(\"{} % Completed.\".format((Frame/length)*100))\n",
        "\n"
      ],
      "metadata": {
        "id": "X-g2ZW3rvDQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbGkDelbdAxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28f6ad9-49b5-47c5-efcf-61a2620bd6c5"
      },
      "source": [
        "run_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99.58333333333333 % Completed.\n",
            "\n",
            "Processing completed.......!!!\n",
            "Check video file in google drive folder!\n"
          ]
        }
      ]
    }
  ]
}